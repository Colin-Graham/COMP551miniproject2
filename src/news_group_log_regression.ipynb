{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model as m\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#stop word sets specified for this data set\n",
    "path = \"../data/stop_words_news_groups/\"\n",
    "with open(path+\"stop_words_no_numbers.txt\", 'r') as content_file:\n",
    "        content = content_file.read().replace(\" \",\"\").replace(\"\\n\",\"\").split(\",\")\n",
    "        stop_words_no_nums = frozenset(content)\n",
    "with open(path+\"stop_words_with_nums.txt\", 'r') as content_file:\n",
    "        content = content_file.read().replace(\" \",\"\").replace(\"\\n\",\"\").split(\",\")\n",
    "        stop_words_with_nums = frozenset(content)\n",
    "# we intend to test each solver independently and then compare them afterwards\n",
    "clf_newton = m.Classifier(1,LogisticRegression(solver ='newton-cg'))\n",
    "clf_saga = m.Classifier(1,LogisticRegression(solver ='saga'))\n",
    "clf_lib = m.Classifier(1,LogisticRegression(solver ='liblinear'))\n",
    "clf_sag = m.Classifier(1,LogisticRegression(solver ='sag'))\n",
    "clf_general = m.Classifier(1,LogisticRegression()) # can be used after to furher improve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_trial_1 = { \n",
    "            #penalty is l2 by default\n",
    "            'vect__max_features': (1000, 10000), # we dont want features to exceed observations (because of overfitting)\n",
    "            'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "            'tfidf__norm': ('l1','l2',None),\n",
    "            'vect__stop_words' : [stop_words_no_nums, stop_words_with_nums],\n",
    "            'clf__max_iter': ([50,150,250]), #two extremes\n",
    "            'clf__C':(10,1.0),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 23.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1441.918s\n",
      "\n",
      "scores!\n",
      "mean: 0.573 std: (+/-0.007) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.581 std: (+/-0.009) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.571 std: (+/-0.006) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.581 std: (+/-0.009) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.706 std: (+/-0.010) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.707 std: (+/-0.010) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.702 std: (+/-0.010) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.704 std: (+/-0.009) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.548 std: (+/-0.007) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.554 std: (+/-0.004) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.545 std: (+/-0.004) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.553 std: (+/-0.006) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.725 std: (+/-0.011) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.722 std: (+/-0.011) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.720 std: (+/-0.014) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.720 std: (+/-0.012) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.382 std: (+/-0.023) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.389 std: (+/-0.023) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.380 std: (+/-0.021) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.389 std: (+/-0.025) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.496 std: (+/-0.026) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.500 std: (+/-0.029) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.494 std: (+/-0.028) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.499 std: (+/-0.030) for {'clf__C': 10, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.573 std: (+/-0.007) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.582 std: (+/-0.009) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.571 std: (+/-0.006) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.580 std: (+/-0.009) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.706 std: (+/-0.010) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.707 std: (+/-0.010) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.702 std: (+/-0.010) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.704 std: (+/-0.010) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.548 std: (+/-0.007) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.555 std: (+/-0.004) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.545 std: (+/-0.004) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.553 std: (+/-0.006) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.725 std: (+/-0.011) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.722 std: (+/-0.011) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.721 std: (+/-0.014) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.720 std: (+/-0.012) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.383 std: (+/-0.023) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.391 std: (+/-0.024) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.381 std: (+/-0.022) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.390 std: (+/-0.025) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.497 std: (+/-0.026) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.502 std: (+/-0.029) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.496 std: (+/-0.028) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.501 std: (+/-0.030) for {'clf__C': 10, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.573 std: (+/-0.007) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.581 std: (+/-0.009) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.571 std: (+/-0.006) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.580 std: (+/-0.009) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.706 std: (+/-0.010) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.707 std: (+/-0.009) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.702 std: (+/-0.010) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.705 std: (+/-0.010) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.548 std: (+/-0.007) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.554 std: (+/-0.004) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.545 std: (+/-0.004) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.553 std: (+/-0.006) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.725 std: (+/-0.010) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.722 std: (+/-0.011) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.721 std: (+/-0.014) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.720 std: (+/-0.012) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.384 std: (+/-0.021) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.392 std: (+/-0.024) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.382 std: (+/-0.021) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.390 std: (+/-0.025) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.499 std: (+/-0.027) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.503 std: (+/-0.029) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.496 std: (+/-0.028) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.501 std: (+/-0.031) for {'clf__C': 10, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.542 std: (+/-0.007) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.551 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.540 std: (+/-0.012) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.550 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.631 std: (+/-0.007) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.633 std: (+/-0.006) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.629 std: (+/-0.010) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.632 std: (+/-0.008) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.576 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.584 std: (+/-0.010) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.575 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.581 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.722 std: (+/-0.016) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.723 std: (+/-0.014) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.718 std: (+/-0.010) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.717 std: (+/-0.010) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.383 std: (+/-0.023) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.390 std: (+/-0.025) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.381 std: (+/-0.022) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.389 std: (+/-0.025) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.495 std: (+/-0.029) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.499 std: (+/-0.028) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.495 std: (+/-0.028) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.499 std: (+/-0.033) for {'clf__C': 1.0, 'clf__max_iter': 50, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.542 std: (+/-0.007) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.551 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.540 std: (+/-0.013) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.550 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.631 std: (+/-0.007) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.633 std: (+/-0.007) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.628 std: (+/-0.011) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.632 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.576 std: (+/-0.010) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.584 std: (+/-0.010) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.575 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.581 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.722 std: (+/-0.016) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.723 std: (+/-0.014) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.718 std: (+/-0.010) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.717 std: (+/-0.010) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.383 std: (+/-0.022) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.391 std: (+/-0.024) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.382 std: (+/-0.021) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.390 std: (+/-0.024) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.497 std: (+/-0.027) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.502 std: (+/-0.029) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.495 std: (+/-0.029) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.500 std: (+/-0.032) for {'clf__C': 1.0, 'clf__max_iter': 150, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.542 std: (+/-0.007) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.551 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.539 std: (+/-0.012) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.550 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.631 std: (+/-0.008) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.633 std: (+/-0.007) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.629 std: (+/-0.011) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.632 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l1', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.576 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.584 std: (+/-0.010) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.575 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.581 std: (+/-0.009) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.722 std: (+/-0.016) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.723 std: (+/-0.014) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.718 std: (+/-0.010) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.718 std: (+/-0.010) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': 'l2', 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.384 std: (+/-0.021) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.392 std: (+/-0.025) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.382 std: (+/-0.021) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.390 std: (+/-0.024) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 1000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "mean: 0.498 std: (+/-0.026) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 0}\n",
      "mean: 0.503 std: (+/-0.028) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 1), 'vect__stop_words': 1}\n",
      "mean: 0.496 std: (+/-0.029) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 0}\n",
      "mean: 0.502 std: (+/-0.031) for {'clf__C': 1.0, 'clf__max_iter': 250, 'tfidf__norm': None, 'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': 1}\n",
      "Best score:\n",
      "0.725 (+/-0.031)\n",
      "with parameters set:\n",
      "\tclf__C: 10\n",
      "\tclf__max_iter: 150\n",
      "\ttfidf__norm: 'l2'\n",
      "\tvect__max_features: 10000\n",
      "\tvect__ngram_range: (1, 1)\n",
      "\tvect__stop_words: 0\n"
     ]
    }
   ],
   "source": [
    "#### TEST 1 looking at saga ####\n",
    "clf_saga.fit(params_trial_1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_saga.eval_on_test([],False)\n",
    "clf_saga.learning_curve([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "clf.eval_best_n_params(0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
