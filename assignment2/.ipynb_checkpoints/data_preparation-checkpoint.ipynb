{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/midou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    '''\n",
    "    add document into vocab Counter\n",
    "    '''\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "def doc_to_line(filename, vocab):\n",
    "    '''\n",
    "    process document + clean + filter and returns it as a line\n",
    "    '''\n",
    "    doc = load_doc(filename)\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "    \n",
    "def clean_doc(doc):\n",
    "    '''\n",
    "    removes punctuation + everything in lowercase + removes '<br/>'\n",
    "    + only consider english words\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = doc.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    # remove punctuation\n",
    "    words = [w.translate(table) for w in words]\n",
    "    # replaces upper case by lower case\n",
    "    words = [word.lower() for word in words]\n",
    "    # remove words that are not alphabetic\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    # remove 'br'\n",
    "    words = [word for word in words if word not in ['br']]  \n",
    "    # removes non-english words\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # filter out short word\n",
    "    words = [word for word in words if len(word) > 1]\n",
    "    return words    \n",
    "\n",
    "def load_doc(filename):\n",
    "    '''\n",
    "    load doc into memory\n",
    "    '''\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def process_docs(directory, vocab):\n",
    "    '''\n",
    "    load each doc in directory\n",
    "    '''\n",
    "    lines = list()\n",
    "    for filename in listdir(directory):\n",
    "        # full file path name\n",
    "        path = directory + '/' + filename\n",
    "        line = doc_to_line(path, vocab)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs_to_vocab(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extensionab\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "        \n",
    "def save_list(lines, filename):\n",
    "    '''\n",
    "    saves tokens to file\n",
    "    '''\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "def remove_min_occ(vocab, occ, filename):\n",
    "    tokens = [k for k, c in vocab.items() if c>=min_occurance]\n",
    "    save_list(tokens, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117232\n",
      "[('movie', 41807), ('film', 37455), ('one', 25508), ('like', 19641), ('good', 14555), ('even', 12503), ('would', 12135), ('time', 11779), ('really', 11663), ('story', 11454), ('see', 11223), ('much', 9584), ('well', 9372), ('get', 9212), ('also', 9073), ('people', 8951), ('bad', 8912), ('great', 8894), ('first', 8857), ('dont', 8473), ('made', 7990), ('movies', 7788), ('make', 7729), ('films', 7727), ('could', 7713), ('way', 7685), ('characters', 7290), ('think', 7229), ('watch', 6777), ('two', 6643), ('many', 6640), ('seen', 6529), ('character', 6514), ('never', 6425), ('little', 6387), ('acting', 6291), ('plot', 6275), ('best', 6263), ('love', 6214), ('know', 6038), ('life', 5988), ('show', 5967), ('ever', 5804), ('still', 5561), ('better', 5547), ('end', 5361), ('say', 5331), ('man', 5211), ('scene', 5169), ('scenes', 5063)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "process_docs_to_vocab('aclImdb/train/neg', vocab)\n",
    "process_docs_to_vocab('aclImdb/train/pos', vocab)\n",
    "print(len(vocab))\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_min_occ(vocab, 5, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load vocab\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare negative reviews\n",
    "negative_lines = process_docs('aclImdb/train/neg', vocab)\n",
    "save_list(negative_lines, 'negative.txt')\n",
    "# prepare positive reviews\n",
    "positive_lines = process_docs('aclImdb/train/pos', vocab)\n",
    "save_list(positive_lines, 'positive.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test negative reviews\n",
    "negative_lines_test = process_docs('aclImdb/test/neg', vocab)\n",
    "save_list(negative_lines_test, 'negative_test.txt')\n",
    "# prepare test positive reviews\n",
    "positive_lines_test = process_docs('aclImdb/test/pos', vocab)\n",
    "save_list(positive_lines_test, 'positive_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
